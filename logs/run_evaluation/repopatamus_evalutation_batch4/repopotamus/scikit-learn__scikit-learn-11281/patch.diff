```diff
--- a/sklearn/mixture/base.py
+++ b/sklearn/mixture/base.py
@@ -188,10 +188,10 @@
         -------
         self
         """
-        X = _check_X(X, self.n_components, ensure_min_samples=2)
-        self._check_initial_parameters(X)
+        return self.fit_predict(X, y)
 
-        # if we enable warm_start, we will have a unique initialisation
+    def fit_predict(self, X, y=None):
+        """Estimate model parameters using X and predict the labels for X.
+
+        The method fits the model n_init times and sets the parameters with
+        which the model has the largest likelihood or lower bound. Within each
+        trial, the method iterates between E-step and M-step for `max_iter`
+        times until the change of likelihood or lower bound is less than
+        `tol`, otherwise, a `ConvergenceWarning` is raised. After fitting, it
+        predicts the most probable label for the input data points.
+
+        .. versionadded:: 0.20
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            List of n_features-dimensional data points. Each row
+            corresponds to a single data point.
+
+        Returns
+        -------
+        labels : array, shape (n_samples,)
+            Component labels.
+        """
+        X = _check_X(X, self.n_components, ensure_min_samples=2)
+        self._check_initial_parameters(X)
+
+        # if we enable warm_start, we will have a unique initialisation
         do_init = not(self.warm_start and hasattr(self, 'converged_'))
         n_init = self.n_init if do_init else 1
 
@@ -237,7 +237,7 @@
         self._set_parameters(best_params)
         self.n_iter_ = best_n_iter
 
-        return self
+        return log_resp.argmax(axis=1)
 
     def _e_step(self, X):
         """E step.
```