2025-05-04 17:23:38,204 - INFO - Creating container for django__django-16816...
2025-05-04 17:23:38,791 - INFO - Container for django__django-16816 created: 40d0d5bc303afacdd161de71158143aadac9aacaf7f712bf74378ac2ed708b22
2025-05-04 17:23:39,043 - INFO - Container for django__django-16816 started: 40d0d5bc303afacdd161de71158143aadac9aacaf7f712bf74378ac2ed708b22
2025-05-04 17:23:39,048 - INFO - Intermediate patch for django__django-16816 written to logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/patch.diff, now applying to container...
2025-05-04 17:23:39,207 - INFO - Failed to apply patch to container: git apply --verbose
2025-05-04 17:23:39,270 - INFO - Failed to apply patch to container: git apply --verbose --reject
2025-05-04 17:23:39,335 - INFO - Failed to apply patch to container: patch --batch --fuzz=5 -p1 -i
2025-05-04 17:23:39,335 - INFO - >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

2025-05-04 17:23:39,623 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: <unprintable EvaluationError object>

2025-05-04 17:23:39,624 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,624 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,624 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,624 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,625 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,625 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,625 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,625 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,626 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,626 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,626 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,626 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,627 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,627 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,627 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,627 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,628 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,628 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,628 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,629 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,629 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,629 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,630 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,630 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,630 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,630 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,631 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,631 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,631 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,631 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,632 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,632 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,632 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,632 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,633 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,633 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,633 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,633 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,634 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,634 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,634 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,635 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,635 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,635 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,635 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,636 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,636 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,636 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,637 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,637 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,637 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,637 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,637 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,638 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,638 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,639 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,639 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,639 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,639 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,640 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,640 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,640 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,640 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,641 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,641 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,642 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,642 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,642 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,642 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,643 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,643 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,643 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,644 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,644 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,644 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,644 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,645 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,645 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,645 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,645 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,646 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,646 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,646 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,647 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,647 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,647 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,647 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,648 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,648 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,648 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,648 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,648 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,649 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,649 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,649 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,650 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,650 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,650 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,650 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,651 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,651 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,651 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,651 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,652 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,652 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,652 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,653 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,653 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,653 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,654 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,654 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,654 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,654 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,655 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,655 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,655 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,656 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,656 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,656 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,657 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,657 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,657 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,957 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: <unprintable EvaluationError object>

2025-05-04 17:23:39,957 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,958 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,958 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,958 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,959 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,959 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,959 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,959 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,960 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,960 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,960 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,961 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,961 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,961 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,962 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,962 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,962 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,963 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,963 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,963 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,963 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,964 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,964 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,964 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,964 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,965 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,965 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,965 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,965 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,966 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,966 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,966 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,966 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,967 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,967 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,967 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,967 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,968 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,968 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,968 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,968 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,969 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,969 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,969 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,969 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,970 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,970 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,970 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,970 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,971 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,971 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,971 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,971 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,972 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,972 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,972 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,973 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,973 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,973 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,974 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,974 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,974 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,975 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,975 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,975 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,975 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,976 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,976 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,976 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,976 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,977 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,977 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,977 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,977 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,978 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,978 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,978 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,978 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,979 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,979 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,979 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,980 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,980 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,980 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,981 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,981 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,981 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,982 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,982 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,982 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,982 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,983 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,983 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,983 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,984 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,984 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,984 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,984 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,985 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,985 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,985 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,985 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,986 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,986 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,986 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,987 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,987 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,987 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,988 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,988 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,988 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,988 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,989 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,989 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,989 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,989 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,989 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,990 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,990 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,990 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,990 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,991 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-16816: >>>>> Patch Apply Failed:
patching file django/contrib/admin/checks.py
Hunk #1 FAILED at 900.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-16816/run_instance.log) for more information.

2025-05-04 17:23:39,991 - INFO - Attempting to stop container sweb.eval.django__django-16816.trial_run_corrected_mode_4...
2025-05-04 17:23:55,794 - INFO - Attempting to remove container sweb.eval.django__django-16816.trial_run_corrected_mode_4...
2025-05-04 17:23:55,827 - INFO - Container sweb.eval.django__django-16816.trial_run_corrected_mode_4 removed.
2025-05-04 17:23:55,828 - INFO - Attempting to remove image swebench/sweb.eval.x86_64.django_1776_django-16816:latest...
2025-05-04 17:23:56,376 - INFO - Image swebench/sweb.eval.x86_64.django_1776_django-16816:latest removed.
