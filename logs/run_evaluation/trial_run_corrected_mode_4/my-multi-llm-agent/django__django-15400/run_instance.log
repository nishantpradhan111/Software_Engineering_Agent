2025-05-04 17:13:59,449 - INFO - Creating container for django__django-15400...
2025-05-04 17:13:59,779 - INFO - Container for django__django-15400 created: 3f36581540cd102dce5ad4bf3467b6d89d502bdc797b1793a3753ed8c7eae8fe
2025-05-04 17:14:00,008 - INFO - Container for django__django-15400 started: 3f36581540cd102dce5ad4bf3467b6d89d502bdc797b1793a3753ed8c7eae8fe
2025-05-04 17:14:00,017 - INFO - Intermediate patch for django__django-15400 written to logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/patch.diff, now applying to container...
2025-05-04 17:14:00,228 - INFO - Failed to apply patch to container: git apply --verbose
2025-05-04 17:14:00,310 - INFO - Failed to apply patch to container: git apply --verbose --reject
2025-05-04 17:14:00,406 - INFO - Failed to apply patch to container: patch --batch --fuzz=5 -p1 -i
2025-05-04 17:14:00,406 - INFO - >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

2025-05-04 17:14:00,682 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: <unprintable EvaluationError object>

2025-05-04 17:14:00,683 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,683 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,683 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,684 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,684 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,684 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,684 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,685 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,685 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,685 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,685 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,686 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,686 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,686 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,687 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,687 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,687 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,687 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,688 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,688 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,688 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,688 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,689 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,689 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,689 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,690 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,690 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,690 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,690 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,691 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,691 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,691 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,692 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,692 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,692 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,692 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,693 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,693 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,693 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,693 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,694 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,694 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,694 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,694 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,695 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,695 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,695 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,696 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,696 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,696 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,696 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,697 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,697 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,697 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,697 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,698 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,698 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,698 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,698 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,699 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,699 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,699 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,699 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,700 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,700 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,700 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,701 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,701 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,701 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,701 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,702 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,702 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,702 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,702 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,703 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,703 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,703 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,703 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,704 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,704 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,704 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,704 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,705 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,705 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,705 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,705 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,706 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,706 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,706 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,707 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,707 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,707 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,708 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,708 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,708 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,708 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,709 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,709 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,709 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,709 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,710 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,710 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,710 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,710 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,711 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,711 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,711 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,711 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,712 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,712 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,713 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,713 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,713 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,713 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,714 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,714 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,714 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,715 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,715 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,715 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,716 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:00,716 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,032 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: <unprintable EvaluationError object>

2025-05-04 17:14:01,032 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,033 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,033 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,033 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,034 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,034 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,034 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,034 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,035 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,035 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,035 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,036 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,036 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,036 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,036 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,036 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,037 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,037 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,038 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,038 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,038 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,038 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,039 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,039 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,039 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,039 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,040 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,040 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,040 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,041 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,041 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,041 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,042 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,042 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,042 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,042 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,043 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,043 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,043 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,044 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,044 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,044 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,045 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,045 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,045 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,046 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,046 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,046 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,047 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,047 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,047 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,048 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,048 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,048 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,048 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,049 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,049 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,049 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,050 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,050 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,050 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,050 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,051 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,051 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,051 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,052 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,052 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,052 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,053 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,053 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,053 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,053 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,054 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,054 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,054 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,055 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,055 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,055 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,055 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,056 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,056 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,056 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,057 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,057 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,057 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,058 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,058 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,058 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,059 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,059 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,059 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,060 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,060 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,060 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,061 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,061 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,061 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,061 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,062 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,062 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,062 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,063 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,063 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,063 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,064 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,064 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,064 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,065 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,065 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,065 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,065 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,066 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,066 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,066 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,066 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,067 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,067 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,067 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,067 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,068 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,068 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,068 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-15400: >>>>> Patch Apply Failed:
patching file django/utils/functional.py
Hunk #1 FAILED at 432.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/utils/functional.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4/my-multi-llm-agent/django__django-15400/run_instance.log) for more information.

2025-05-04 17:14:01,068 - INFO - Attempting to stop container sweb.eval.django__django-15400.trial_run_corrected_mode_4...
2025-05-04 17:14:16,511 - INFO - Attempting to remove container sweb.eval.django__django-15400.trial_run_corrected_mode_4...
2025-05-04 17:14:16,543 - INFO - Container sweb.eval.django__django-15400.trial_run_corrected_mode_4 removed.
2025-05-04 17:14:16,543 - INFO - Attempting to remove image swebench/sweb.eval.x86_64.django_1776_django-15400:latest...
2025-05-04 17:14:17,020 - INFO - Image swebench/sweb.eval.x86_64.django_1776_django-15400:latest removed.
