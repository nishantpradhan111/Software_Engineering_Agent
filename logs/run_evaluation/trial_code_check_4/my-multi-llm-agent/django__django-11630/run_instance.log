2025-05-08 17:11:25,964 - INFO - Creating container for django__django-11630...
2025-05-08 17:11:26,867 - INFO - Container for django__django-11630 created: 4fb3b82d7ec050030f00ccef5a773088a76155e90bf338fa24528d4e417f1229
2025-05-08 17:11:27,169 - INFO - Container for django__django-11630 started: 4fb3b82d7ec050030f00ccef5a773088a76155e90bf338fa24528d4e417f1229
2025-05-08 17:11:27,176 - INFO - Intermediate patch for django__django-11630 written to logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/patch.diff, now applying to container...
2025-05-08 17:11:27,376 - INFO - Failed to apply patch to container: git apply --verbose
2025-05-08 17:11:27,459 - INFO - Failed to apply patch to container: git apply --verbose --reject
2025-05-08 17:11:27,533 - INFO - Failed to apply patch to container: patch --batch --fuzz=5 -p1 -i
2025-05-08 17:11:27,534 - INFO - >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

2025-05-08 17:11:27,949 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: <unprintable EvaluationError object>

2025-05-08 17:11:27,949 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,950 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,950 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,951 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,951 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,952 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,952 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,952 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,952 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,953 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,953 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,953 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,954 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,954 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,954 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,954 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,955 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,955 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,955 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,956 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,956 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,956 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,956 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,957 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,957 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,957 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,958 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,958 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,958 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,958 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,959 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,959 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,959 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,959 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,960 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,960 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,961 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,961 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,961 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,961 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,962 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,962 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,962 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,963 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,963 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,963 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,963 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,964 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,964 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,964 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,965 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,965 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,965 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,965 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,966 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,966 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,966 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,967 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,967 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,967 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,968 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,968 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,968 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,968 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,969 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,969 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,969 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,970 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,970 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,970 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,970 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,971 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,971 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,971 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,971 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,972 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,972 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,972 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,973 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,973 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,973 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,974 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,974 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,974 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,975 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,975 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,976 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,976 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,976 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,977 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,977 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,978 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,978 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,978 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,980 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,980 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,981 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,981 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,981 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,982 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,982 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,983 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,983 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,984 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,985 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,985 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,986 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,987 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,987 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,989 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,990 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,990 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,993 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,995 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:27,999 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,002 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,003 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,004 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,005 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,006 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,007 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,008 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,402 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: <unprintable EvaluationError object>

2025-05-08 17:11:28,402 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,403 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,403 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,403 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,404 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,404 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,404 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,405 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,405 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,405 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,406 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,406 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,406 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,407 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,407 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,407 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,407 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,408 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,408 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,409 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,410 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,411 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,418 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,419 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,419 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,420 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,420 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,421 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,422 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,422 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,422 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,423 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,424 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,424 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,425 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,425 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,425 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,426 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,426 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,426 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,427 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,428 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,428 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,428 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,428 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,429 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,429 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,429 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,430 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,430 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,430 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,430 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,431 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,431 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,431 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,432 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,432 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,432 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,432 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,433 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,433 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,433 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,434 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,434 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,434 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,435 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,435 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,435 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,435 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,436 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,436 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,436 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,437 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,437 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,437 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,437 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,438 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,438 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,438 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,439 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,439 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,439 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,440 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,440 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,441 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,441 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,441 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,442 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,442 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,443 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,443 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,443 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,444 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,444 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,444 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,445 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,445 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,445 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,445 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,446 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,446 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,446 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,447 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,447 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,447 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,448 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,448 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,448 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,449 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,449 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,449 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,449 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,450 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,450 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,450 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,451 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,452 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,452 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,453 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,454 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,454 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,454 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_4/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 17:11:28,455 - INFO - Attempting to stop container sweb.eval.django__django-11630.trial_code_check_4...
2025-05-08 17:11:45,034 - INFO - Attempting to remove container sweb.eval.django__django-11630.trial_code_check_4...
2025-05-08 17:11:45,141 - INFO - Container sweb.eval.django__django-11630.trial_code_check_4 removed.
2025-05-08 17:11:45,142 - INFO - Attempting to remove image swebench/sweb.eval.x86_64.django_1776_django-11630:latest...
2025-05-08 17:11:45,968 - INFO - Image swebench/sweb.eval.x86_64.django_1776_django-11630:latest removed.
