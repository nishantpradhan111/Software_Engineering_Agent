2025-05-07 18:56:24,331 - INFO - Creating container for django__django-11630...
2025-05-07 18:56:25,409 - INFO - Container for django__django-11630 created: 5bdf0aeec03f1be0f8dadb813ed578f470650e12acc43ea482e020ef68ad6ed6
2025-05-07 18:56:26,021 - INFO - Container for django__django-11630 started: 5bdf0aeec03f1be0f8dadb813ed578f470650e12acc43ea482e020ef68ad6ed6
2025-05-07 18:56:26,032 - INFO - Intermediate patch for django__django-11630 written to logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/patch.diff, now applying to container...
2025-05-07 18:56:26,387 - INFO - Failed to apply patch to container: git apply --verbose
2025-05-07 18:56:26,531 - INFO - Failed to apply patch to container: git apply --verbose --reject
2025-05-07 18:56:26,695 - INFO - Failed to apply patch to container: patch --batch --fuzz=5 -p1 -i
2025-05-07 18:56:26,696 - INFO - >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

2025-05-07 18:56:27,407 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: <unprintable EvaluationError object>

2025-05-07 18:56:27,407 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,408 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,408 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,409 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,410 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,411 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,411 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,412 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,413 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,413 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,414 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,414 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,415 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,416 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,417 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,418 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,419 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,419 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,420 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,421 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,421 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,422 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,423 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,423 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,424 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,425 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,425 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,426 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,427 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,428 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,428 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,429 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,430 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,430 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,431 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,432 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,432 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,433 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,434 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,434 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,435 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,436 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,437 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,438 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,439 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,440 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,441 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,442 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,442 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,443 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,444 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,444 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,445 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,446 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,447 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,447 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,448 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,449 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,449 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,450 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,451 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,452 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,452 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,453 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,454 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,455 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,455 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,456 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,456 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,457 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,458 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,458 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,459 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,459 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,460 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,461 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,462 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,462 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,462 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,463 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,463 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,464 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,465 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,465 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,466 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,467 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,467 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,468 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,468 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,469 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,469 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,470 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,471 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,471 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,472 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,473 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,473 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,474 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,475 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,475 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,476 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,476 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,477 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,477 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,478 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,478 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,479 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,479 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,480 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,480 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,481 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,481 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,482 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,482 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,483 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,484 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,484 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,485 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,485 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,486 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,486 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:27,487 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,134 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: <unprintable EvaluationError object>

2025-05-07 18:56:28,135 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,136 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,136 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,137 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,137 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,138 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,139 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,139 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,140 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,141 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,141 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,142 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,142 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,143 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,144 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,144 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,145 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,146 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,146 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,147 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,148 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,148 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,149 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,150 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,151 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,152 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,153 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,153 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,154 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,155 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,155 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,156 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,156 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,157 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,158 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,158 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,159 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,160 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,160 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,161 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,162 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,162 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,163 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,164 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,165 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,165 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,166 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,167 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,168 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,168 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,169 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,169 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,170 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,171 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,171 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,172 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,172 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,173 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,177 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,178 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,178 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,179 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,180 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,181 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,182 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,183 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,184 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,185 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,185 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,186 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,186 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,187 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,188 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,188 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,189 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,189 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,190 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,191 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,191 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,192 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,192 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,193 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,194 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,194 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,195 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,196 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,196 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,197 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,198 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,198 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,199 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,199 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,201 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,201 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,202 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,202 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,203 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,204 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,204 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,205 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,206 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,206 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,207 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,207 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,208 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,209 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,209 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,210 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,211 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,211 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,212 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,213 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,213 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,214 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,215 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,215 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,216 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,216 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,217 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,218 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,218 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,219 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 succeeded at 4 with fuzz 3 (offset -1 lines).
Hunk #2 FAILED at 36.
patch unexpectedly ends in middle of line
1 out of 2 hunks FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_4.5/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-07 18:56:28,220 - INFO - Attempting to stop container sweb.eval.django__django-11630.trial_run_corrected_mode_4.5...
2025-05-07 18:56:44,011 - INFO - Attempting to remove container sweb.eval.django__django-11630.trial_run_corrected_mode_4.5...
2025-05-07 18:56:44,064 - INFO - Container sweb.eval.django__django-11630.trial_run_corrected_mode_4.5 removed.
2025-05-07 18:56:44,066 - INFO - Attempting to remove image swebench/sweb.eval.x86_64.django_1776_django-11630:latest...
2025-05-07 18:56:45,296 - INFO - Image swebench/sweb.eval.x86_64.django_1776_django-11630:latest removed.
