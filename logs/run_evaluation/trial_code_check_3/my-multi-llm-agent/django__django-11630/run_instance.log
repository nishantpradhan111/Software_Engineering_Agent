2025-05-08 16:56:04,528 - INFO - Creating container for django__django-11630...
2025-05-08 16:56:05,388 - INFO - Container for django__django-11630 created: c8150f15f44ef44b9599b3d65abe4b2022ded599904f4edcf178cb51e32e88c5
2025-05-08 16:56:05,697 - INFO - Container for django__django-11630 started: c8150f15f44ef44b9599b3d65abe4b2022ded599904f4edcf178cb51e32e88c5
2025-05-08 16:56:05,706 - INFO - Intermediate patch for django__django-11630 written to logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/patch.diff, now applying to container...
2025-05-08 16:56:05,927 - INFO - Failed to apply patch to container: git apply --verbose
2025-05-08 16:56:06,018 - INFO - Failed to apply patch to container: git apply --verbose --reject
2025-05-08 16:56:06,103 - INFO - Failed to apply patch to container: patch --batch --fuzz=5 -p1 -i
2025-05-08 16:56:06,104 - INFO - >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

2025-05-08 16:56:06,781 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: <unprintable EvaluationError object>

2025-05-08 16:56:06,782 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,782 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,782 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,783 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,783 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,783 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,784 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,784 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,784 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,785 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,786 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,786 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,786 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,787 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,787 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,788 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,788 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,788 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,789 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,794 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,795 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,795 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,796 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,796 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,796 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,797 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,797 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,797 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,798 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,798 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,799 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,799 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,799 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,799 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,800 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,800 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,800 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,801 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,801 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,801 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,802 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,802 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,802 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,803 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,803 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,803 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,804 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,804 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,804 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,805 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,806 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,806 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,806 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,807 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,807 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,809 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,810 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,811 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,812 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,813 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,813 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,814 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,814 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,815 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,815 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,817 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,817 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,818 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,818 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,819 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,819 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,820 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,820 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,822 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,823 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,823 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,823 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,824 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,824 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,825 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,825 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,825 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,826 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,826 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,827 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,827 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,828 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,830 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,831 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,831 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,832 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,832 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,833 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,833 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,834 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,834 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,835 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,835 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,836 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,836 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,837 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,837 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,837 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,838 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,838 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,838 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,839 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,840 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,840 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,841 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,841 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,842 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,842 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,842 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,843 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,843 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,843 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,844 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,844 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,846 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,847 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:06,847 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,307 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: <unprintable EvaluationError object>

2025-05-08 16:56:07,307 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,307 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,308 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,308 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,309 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,309 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,309 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,310 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,310 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,310 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,311 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,311 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,311 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,312 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,312 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,313 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,313 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,314 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,314 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,314 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,315 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,315 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,315 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,315 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,316 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,316 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,316 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,317 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,317 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,317 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,318 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,318 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,318 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,320 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,321 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,321 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,321 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,322 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,322 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,322 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,323 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,323 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,323 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,324 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,324 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,324 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,325 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,325 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,326 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,326 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,326 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,327 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,327 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,327 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,328 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,328 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,328 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,329 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,329 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,329 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,330 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,330 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,330 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,331 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,331 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,331 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,332 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,332 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,333 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,333 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,333 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,334 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,334 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,335 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,335 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,336 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,337 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,338 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,338 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,339 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,339 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,339 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,340 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,340 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,340 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,341 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,341 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,341 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,342 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,343 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,343 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,343 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,344 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,344 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,345 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,345 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,346 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,346 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,346 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,347 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,347 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,348 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,348 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,349 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,350 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,351 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,352 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,353 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,356 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,356 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,357 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,358 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,358 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,359 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,360 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,360 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,361 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,362 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,363 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,363 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,364 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-08 16:56:07,364 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/db/models/options.py
Hunk #1 FAILED at 211.
patch unexpectedly ends in middle of line
1 out of 1 hunk FAILED -- saving rejects to file django/db/models/options.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_code_check_3/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

