2025-05-08 15:25:17,516 - INFO - Creating container for django__django-12308...
2025-05-08 15:25:18,000 - INFO - Container for django__django-12308 created: ca21c09acb65a0714aea9b898a4fcc2fc3e2270920ca757ae47d34bc31c6f92f
2025-05-08 15:25:18,377 - INFO - Container for django__django-12308 started: ca21c09acb65a0714aea9b898a4fcc2fc3e2270920ca757ae47d34bc31c6f92f
2025-05-08 15:25:18,383 - INFO - Intermediate patch for django__django-12308 written to logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/patch.diff, now applying to container...
2025-05-08 15:25:18,605 - INFO - Failed to apply patch to container: git apply --verbose
2025-05-08 15:25:18,705 - INFO - Failed to apply patch to container: git apply --verbose --reject
2025-05-08 15:25:18,796 - INFO - Failed to apply patch to container: patch --batch --fuzz=5 -p1 -i
2025-05-08 15:25:18,796 - INFO - >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

2025-05-08 15:25:19,240 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: <unprintable EvaluationError object>

2025-05-08 15:25:19,241 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,241 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,241 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,242 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,242 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,242 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,243 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,243 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,244 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,244 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,244 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,245 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,245 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,245 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,246 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,246 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,246 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,247 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,247 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,247 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,248 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,248 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,249 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,249 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,250 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,250 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,250 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,251 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,251 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,252 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,252 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,252 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,253 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,254 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,254 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,255 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,255 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,255 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,256 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,256 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,256 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,257 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,257 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,258 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,258 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,258 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,259 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,259 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,259 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,260 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,260 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,261 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,261 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,261 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,262 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,262 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,262 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,263 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,263 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,263 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,264 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,264 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,264 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,265 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,265 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,265 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,266 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,266 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,267 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,267 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,268 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,268 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,268 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,269 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,269 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,270 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,270 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,270 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,271 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,271 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,271 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,272 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,272 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,273 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,273 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,273 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,274 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,274 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,274 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,275 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,275 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,275 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,277 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,277 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,277 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,278 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,278 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,278 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,279 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,279 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,279 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,280 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,280 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,281 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,281 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,282 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,282 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,282 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,283 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,283 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,284 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,284 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,284 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,285 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,285 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,286 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,286 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,286 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,287 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,287 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,287 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,288 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,712 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: <unprintable EvaluationError object>

2025-05-08 15:25:19,712 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,713 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,713 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,713 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,714 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,714 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,715 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,715 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,715 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,716 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,716 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,716 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,717 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,717 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,718 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,718 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,718 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,718 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,719 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,719 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,719 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,720 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,720 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,720 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,722 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,723 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,723 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,723 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,724 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,724 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,725 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,725 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,725 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,726 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,728 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,729 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,730 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,731 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,732 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,733 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,734 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,734 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,735 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,735 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,736 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,736 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,736 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,737 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,737 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,738 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,738 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,738 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,739 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,739 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,740 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,740 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,741 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,741 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,741 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,742 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,742 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,742 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,743 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,743 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,744 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,744 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,745 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,745 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,745 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,746 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,746 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,747 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,747 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,747 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,748 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,748 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,749 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,749 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,749 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,750 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,750 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,751 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,751 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,752 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,752 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,752 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,753 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,753 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,753 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,754 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,754 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,754 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,755 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,755 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,755 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,756 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,756 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,757 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,757 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,758 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,758 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,759 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,759 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,760 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,760 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,761 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,761 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,761 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,762 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,762 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,763 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,763 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,763 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,764 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,764 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,765 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,765 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,765 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,766 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,766 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,767 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,767 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-12308: >>>>> Patch Apply Failed:
patching file django/contrib/admin/utils.py
Hunk #1 FAILED at 144.
1 out of 1 hunk FAILED -- saving rejects to file django/contrib/admin/utils.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_mode_8/my-multi-llm-agent/django__django-12308/run_instance.log) for more information.

2025-05-08 15:25:19,768 - INFO - Attempting to stop container sweb.eval.django__django-12308.trial_run_corrected_mode_8...
2025-05-08 15:25:35,778 - INFO - Attempting to remove container sweb.eval.django__django-12308.trial_run_corrected_mode_8...
2025-05-08 15:25:35,825 - INFO - Container sweb.eval.django__django-12308.trial_run_corrected_mode_8 removed.
2025-05-08 15:25:35,827 - INFO - Attempting to remove image swebench/sweb.eval.x86_64.django_1776_django-12308:latest...
2025-05-08 15:25:36,747 - INFO - Image swebench/sweb.eval.x86_64.django_1776_django-12308:latest removed.
