2025-05-04 09:32:52,526 - INFO - Creating container for django__django-11630...
2025-05-04 09:32:52,885 - INFO - Container for django__django-11630 created: 12ce4e809a61f1bfec09ddb39f4c1f6126b1a3bec43a83aaa6e9060268bb5c8e
2025-05-04 09:32:53,141 - INFO - Container for django__django-11630 started: 12ce4e809a61f1bfec09ddb39f4c1f6126b1a3bec43a83aaa6e9060268bb5c8e
2025-05-04 09:32:53,148 - INFO - Intermediate patch for django__django-11630 written to logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/patch.diff, now applying to container...
2025-05-04 09:32:53,293 - INFO - Failed to apply patch to container: git apply --verbose
2025-05-04 09:32:53,353 - INFO - Failed to apply patch to container: git apply --verbose --reject
2025-05-04 09:32:53,420 - INFO - Failed to apply patch to container: patch --batch --fuzz=5 -p1 -i
2025-05-04 09:32:53,420 - INFO - >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

2025-05-04 09:32:53,643 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: <unprintable EvaluationError object>

2025-05-04 09:32:53,643 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,643 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,644 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,644 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,644 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,644 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,645 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,645 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,645 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,645 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,646 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,646 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,646 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,646 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,647 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,647 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,647 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,647 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,648 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,648 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,648 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,649 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,649 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,649 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,649 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,650 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,650 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,650 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,650 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,651 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,651 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,651 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,651 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,652 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,652 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,652 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,652 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,653 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,653 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,653 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,653 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,654 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,654 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,654 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,655 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,655 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,655 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,656 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,656 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,656 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,657 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,657 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,657 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,657 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,658 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,658 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,658 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,658 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,659 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,659 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,659 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,659 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,660 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,660 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,660 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,660 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,661 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,661 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,661 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,662 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,662 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,662 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,662 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,663 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,663 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,663 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,663 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,663 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,664 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,664 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,664 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,664 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,665 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,665 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,665 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,665 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,666 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,666 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,666 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,666 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,667 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,667 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,667 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,667 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,668 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,668 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,668 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,668 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,669 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,669 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,669 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,669 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,670 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,670 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,670 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,671 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,671 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,671 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,671 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,672 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,672 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,672 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,673 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,673 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,673 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,673 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,674 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,674 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,674 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,675 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,675 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,675 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,964 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: <unprintable EvaluationError object>

2025-05-04 09:32:53,964 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,965 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,965 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,965 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,966 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,966 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,966 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,966 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,967 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,967 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,967 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,967 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,968 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,968 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,968 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,968 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,968 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,969 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,969 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,969 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,970 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,970 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,971 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,971 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,971 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,972 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,972 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,972 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,972 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,973 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,973 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,973 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,973 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,973 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,974 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,974 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,974 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,975 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,975 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,975 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,976 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,976 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,976 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,977 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,977 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,977 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,978 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,978 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,978 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,978 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,979 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,979 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,979 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,979 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,980 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,980 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,980 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,980 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,981 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,981 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,981 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,981 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,982 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,982 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,982 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,983 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,983 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,983 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,984 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,984 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,984 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,984 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,984 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,985 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,985 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,985 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,985 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,986 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,986 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,986 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,986 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,987 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,987 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,987 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,988 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,988 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,988 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,989 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,989 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,989 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,990 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,990 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,990 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,990 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,991 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,991 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,991 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,991 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,992 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,992 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,992 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,992 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,993 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,993 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,993 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,993 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,994 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,994 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,994 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,995 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,995 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,995 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,995 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,996 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,996 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,996 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,996 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,997 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,997 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,997 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,997 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,998 - INFO - Traceback (most recent call last):
  File "/mnt/c/Nishant/Work/BITS - Engineering/2-2/LLM/Proj/SWE-bench/swebench/harness/run_evaluation.py", line 170, in run_instance
    raise EvaluationError(
swebench.harness.utils.EvaluationError: django__django-11630: >>>>> Patch Apply Failed:
patching file django/core/checks/model_checks.py
Hunk #1 FAILED at 37.
1 out of 1 hunk FAILED -- saving rejects to file django/core/checks/model_checks.py.rej
patch unexpectedly ends in middle of line

Check (logs/run_evaluation/trial_run_corrected_model_2/my-multi-llm-agent/django__django-11630/run_instance.log) for more information.

2025-05-04 09:32:53,998 - INFO - Attempting to stop container sweb.eval.django__django-11630.trial_run_corrected_model_2...
2025-05-04 09:33:09,515 - INFO - Attempting to remove container sweb.eval.django__django-11630.trial_run_corrected_model_2...
2025-05-04 09:33:09,546 - INFO - Container sweb.eval.django__django-11630.trial_run_corrected_model_2 removed.
2025-05-04 09:33:09,546 - INFO - Attempting to remove image swebench/sweb.eval.x86_64.django_1776_django-11630:latest...
2025-05-04 09:33:10,045 - INFO - Image swebench/sweb.eval.x86_64.django_1776_django-11630:latest removed.
