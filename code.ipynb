{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36648678",
   "metadata": {},
   "source": [
    "**The following code only generates the patches and stores them in a file (preds.jsonl)**\n",
    "\n",
    "Follow the steps below to evaluate the generated patches using SWE-Bench:\n",
    "\n",
    "1. *Clone the SWE Bench repository*\n",
    "2. *Install all the required packages in edit mode*\n",
    "3. *Run Docker*\n",
    "4. *Ensure this code file is placed *outside* the cloned SWE Bench repository*\n",
    "5. *Open the terminal in the cloned SWE Bench repository*\n",
    "6. *Run the built-in SWE-Bench evaluator using the following command:*\n",
    "\n",
    "    ```bash\n",
    "    python -m swebench.harness.run_evaluation \\\n",
    "        dataset_name princeton-nlp/SWE-bench_Lite \\\n",
    "        predictions_path ../my_preds.jsonl \\\n",
    "        max_workers 8 \\\n",
    "        run_id testing\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8de96daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage\n",
    "from typing import Dict\n",
    "import json\n",
    "\n",
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191fcb1f",
   "metadata": {},
   "source": [
    "Creating an LLM object using the model Gemini 2.0 Flash and setting temperature 1e-3 (seemed to work best after trying multiple values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f182190",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=\"\",\n",
    "    temperature=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da166a",
   "metadata": {},
   "source": [
    "Function to refine a generated patch, the system prompt ensures that the prompt is in proper unified-diff format and does not have any unnecessary newlines or whitespaces. This is done to prevent \"malformed patch\" errors during evaluation. The function takes the patch and the necessary code as input and outputs a formatted patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e545c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_patch_format(patch,code):\n",
    "    \"\"\"\n",
    "    Ensures a patch is in valid unified-diff format. If malformed, uses an LLM to correct it.\n",
    "    \"\"\"\n",
    "    if not patch.startswith('--- a/') or '@@' not in patch:\n",
    "        prompt = f\"\"\"You are a software engineer fixing an automated code patch.\n",
    "\n",
    "The following patch is malformed or incomplete: {patch}\n",
    "Here is the original code which needs to be patched {code}\n",
    "\n",
    "Please return a corrected patch using valid unified-diff format:\n",
    "- Make sure the whitespaces match the original file\n",
    "- Make sure the context lines of the patch point to the same lines in the code\n",
    "- It must start with '--- a/...' and '+++ b/...'\n",
    "- It must contain at least one valid hunk beginning with '@@'\n",
    "- Do not include explanations, comments, markdown, or any text outside the patch.\n",
    "- Your response must be a pure, corrected unified diff.\n",
    "- It should be in raw unified-diff format, without any markdown wrapping\n",
    "\n",
    "Be as minimalistic as possible, but do not let it affect correctness. Just Check cross verify with the input and try to rmeove redudant lines of code.\n",
    "Return a valid patch only.\"\"\"\n",
    "        feedback = llm([HumanMessage(content=prompt)]).content\n",
    "        return feedback\n",
    "    return patch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612eed65",
   "metadata": {},
   "source": [
    "Function to verify that the line numbers being changed by the patch are accurate. The system prompt instructs the LLM to check the given patch against the original code and ensure that the code being changed by the patch is actually present in the file at the specified line numbers. This helps to ensure that the patch is properly applied during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6fea3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_line_numbers(instance: Dict,llm,patch,code):\n",
    "    prompt=f\"\"\"\n",
    "You are working with a group of software engineers. They have generated a patch {patch} for the problem statement {instance['problem_statement']}.\n",
    "\n",
    "Your job is to make sure their code has the correct line numbers and the patch actually runs. Do not try to change the code or its logic, just make it so that patch runs.\n",
    "\n",
    "All the relevant info for the problem statement is here {instance}.\n",
    "Here is the code {code}. \n",
    "Do the above changes for the patch {patch}\n",
    "    \"\"\"\n",
    "    return llm([HumanMessage(content=prompt)]).content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bcf41e",
   "metadata": {},
   "source": [
    "This function outputs a fully formed patch for the instance given as input. The system prompt gives the LLM access to the problem statement, original code, and the necessary test cases and instructs it to generate a patch satisfying all the specified conditions. \n",
    "The patch initially generated is recursively passed through the reflection loop 3 times. The reflection prompt gives the LLM the same information as earlier and focuses on rechecking the patch for any logical or formatting errors. The output of each call is passed back to itself thrice.\n",
    "The output of the reflection loop is then recursively passed to the check_line_numbers function three times. \n",
    "Finally, the output of the above loop is passed to the refine_patch function as detailed above.\n",
    "The output of the refine_patch function is the final patch generated by the model and it is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patch(instance: Dict, code, llm, max_reflections: int = 3):\n",
    "    #Initial system prompt\n",
    "    \"\"\"\n",
    "    Generate a patch with iterative self-reflection using an LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    problem = instance['problem_statement']\n",
    "    failtopass = instance['FAIL_TO_PASS']\n",
    "    passtopass = instance['PASS_TO_PASS']\n",
    "\n",
    "    initial_prompt = f\"\"\"You are an expert software engineer.\n",
    "\n",
    "Below is a problem description from a GitHub issue:\n",
    "{problem}\n",
    "\n",
    "Here is the code you have to edit {code}\n",
    "These are the error which you should check for{failtopass}\n",
    "These SHOULD NOT fail {passtopass}\n",
    "Check all of the above one by one.\n",
    "\n",
    "Generate a fix for this issue in valid unified-diff format.\n",
    "\n",
    "Make sure to fix each and every error. Go Through all the lines pertaining to the problem and check. Try simulating a test case yourself to see if the code you have written gives ocrrect output.\n",
    "Make sure to check the file path and make sure it is correct to avoid context mismatch errors \n",
    "\n",
    "Your output must:\n",
    "- Be in pure raw unified diff format. There should be no wrappers like the ```\n",
    "- Begin with '--- a/...' and '+++ b/...'\n",
    "- Contain at least one '@@' hunk\n",
    "- NOT include explanations, comments, or markdown\n",
    "- Be syntactically correct\n",
    "- You may add code if its absolutely necessary (like missing write function for read function)\n",
    "- Make sure the code which you are changing (the - part in the code) is actually there\n",
    "- Check if the indices of the line of code and the actual code in the repository actually match\n",
    "\n",
    "If the above does not work, try again.\n",
    "Here is the code you have to edit {code}\n",
    "\n",
    "\n",
    "Output only the patch.\"\"\"\n",
    "    current_patch = llm([HumanMessage(content=initial_prompt)]).content\n",
    "\n",
    "    #Reflection loop\n",
    "    for i in range(max_reflections):\n",
    "        reflection_prompt = f\"\"\"You are reviewing the following patch:\n",
    "{current_patch}\n",
    "The patch was intended to fix this issue:{problem}\n",
    "These are the error which you should check for{failtopass}\n",
    "These SHOULD NOT fail {passtopass}\n",
    "Check all of the above one by one.\n",
    "Here is the code you have to edit {code}\n",
    "\n",
    "Make sure to fix each and every error. Go Through all the lines pertaining to the problem and check. Try simulating a test case yourself to see if the code you have written gives ocrrect output.\n",
    "\n",
    "Check:\n",
    "- Be in pure raw unified diff format. There should be no wrappers like the ```\n",
    "- Does it correctly address the issue?\n",
    "- Is it syntactically valid unified-diff (starts with --- a/, contains @@)?\n",
    "- Is every edit/newline leading with a + or -?\n",
    "- Can it be applied cleanly (no formatting or logical errors)?\n",
    "- Make sure the code which you are changing (the - part in the code) is actually there\n",
    "- Check if the indices of the line of code and the actual code in the repository actually \n",
    "- You may add code if its absolutely necessary (like missing write function for read function)\n",
    "\n",
    "Make sure that when you generate a patch, the changed code module names, variable names, everything matches how it was before so that there is no error. Check for line numbers and everythign very thoroughly.\n",
    "Here is the code you have to edit {code}\n",
    "\n",
    "These are the error which you should check for{failtopass}\n",
    "These SHOULD NOT fail {passtopass}\n",
    "Check all of the above one by one.\n",
    "Make sure to fix each and every error. Go Through all the lines pertaining to the problem and check. Try simulating a test case yourself to see if the code you have written gives ocrrect output.\n",
    "Make sure to check the file path and make sure it is correct to avoid context mismatch errors \n",
    "If any issue is found, rewrite the patch and return only the corrected unified diff. Do not include any explanations or markdown.\"\"\"\n",
    "        current_patch = llm([HumanMessage(content=reflection_prompt)]).content\n",
    "\n",
    "    #Context matching\n",
    "    for i in range(max_reflections):\n",
    "        current_patch = check_line_numbers(instance,llm,current_patch,code)\n",
    "\n",
    "    #Patch refining\n",
    "    current_patch = refine_patch_format(current_patch,code)\n",
    "\n",
    "    #Final patch is returned\n",
    "    return current_patch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414dcdfc",
   "metadata": {},
   "source": [
    "The file_extractor function receives an instance as input. It uses the \"repo\" field of the instance to identify the repository to which the instance belongs. The exact file name is extracted from the \"patch\" field using string formatting. These two components are put together to form the full file path that the model needs to access.\n",
    "The \"base_commit\" field specifies the commit version of the repository that the issue belonged to. The \"git checkout\" command is used to restore the required file to the required version so that the model may have access to the correct version.\n",
    "Once checkout is complete, the file is read and its full contents are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7704e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_extractor(instance):\n",
    "    #Extracting repository name\n",
    "    _, _, tail = instance['repo'].partition(\"/\")\n",
    "    repo_path = \"./\" + tail\n",
    "\n",
    "    #Handling unusual repository name\n",
    "    if(instance['repo'] == 'pallets/flask'):\n",
    "        repo_path = \"./my-flask-project\"\n",
    "\n",
    "    #Extracting commit version\n",
    "    commit_ver  = instance[\"base_commit\"]    \n",
    "\n",
    "    #Extracting file name and forming file path\n",
    "    s = instance['patch']\n",
    "    substring_to_first_py = s[:s.find('.py') + 3]\n",
    "    file_rel = substring_to_first_py.replace(\"diff --git a/\",\"\",1).strip()\n",
    "\n",
    "    #Gut checkout command\n",
    "    subprocess.run(\n",
    "        [\"git\", \"checkout\", commit_ver],\n",
    "        cwd=repo_path,\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "    #Reading and returning file contents\n",
    "    file_path = os.path.join(repo_path, file_rel)\n",
    "    with open(file_path, \"r\") as f:\n",
    "        contents = f.read() \n",
    "\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2707590b",
   "metadata": {},
   "source": [
    "The SWE-bench dataset is loaded using the huggingface module available in Python. Only the test split is accessed, the dev split is not needed. The \"hints\" and \"test_patch\" columns are dropped from the dataset because these details are not needed by the model.\n",
    "All the columns of the dataset are printed for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac4579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['repo', 'instance_id', 'base_commit', 'patch', 'problem_statement', 'hints_text', 'created_at', 'version', 'FAIL_TO_PASS', 'PASS_TO_PASS', 'environment_setup_commit']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"princeton-nlp/SWE-bench_Lite\", split=\"test\")\n",
    "dataset = dataset.remove_columns(\"test_patch\")\n",
    "dataset = dataset.remove_columns(\"hints_text\")\n",
    "\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb9cb68",
   "metadata": {},
   "source": [
    "Each of 300 instances is extracted and passed to generate_patch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9871e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "count = 0\n",
    "for i in range (10):\n",
    "    instance = dataset[i]\n",
    "    try :\n",
    "        code = file_extractor(instance)\n",
    "    except :\n",
    "        print(instance)\n",
    "        continue\n",
    "\n",
    "    patch = generate_patch(instance,code,llm)\n",
    "    predictions.append({\n",
    "        \"instance_id\": instance[\"instance_id\"],\n",
    "        \"model_name_or_path\": \"repopotamus\",\n",
    "        \"model_patch\": patch\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983bcb5e",
   "metadata": {},
   "source": [
    "Writing all the patches along with instance ids to the prediction file to be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"preds.jsonl\", \"w\") as f:\n",
    "    for p in predictions:\n",
    "        f.write(json.dumps(p) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
